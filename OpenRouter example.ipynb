{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17546,
     "status": "ok",
     "timestamp": 1759566330709,
     "user": {
      "displayName": "Greg K.",
      "userId": "05140773900542914628"
     },
     "user_tz": -180
    },
    "id": "WR3zuuh9iT9H",
    "outputId": "c0fa7cad-5c3d-496c-8ae1-be1798855a98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.48.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.8.0+cu126)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (25.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n",
      "Downloading bitsandbytes-0.48.1-py3-none-manylinux_2_24_x86_64.whl (60.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.48.1\n"
     ]
    }
   ],
   "source": [
    "!pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14704,
     "status": "ok",
     "timestamp": 1759570992837,
     "user": {
      "displayName": "Greg K.",
      "userId": "05140773900542914628"
     },
     "user_tz": -180
    },
    "id": "yHEdKb1WixCm",
    "outputId": "7f620af9-511b-4866-881b-ab0f9d4e24f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"score\": 100, \"rationale\": \"Directly addresses the topic of baking a cake.\"}\n",
      "{\"score\": 100, \"rationale\": \"Directly addresses the topic of baking a cake.\"}\n",
      "{\"score\": 95, \"rationale\": \"Directly addresses the topic of baking a cake and the specific step of frosting\"}\n",
      "{\"score\": 10, \"rationale\": \"Barely related.\"}\n",
      "{\"score\": 10, \"rationale\": \"Barely related.\"}\n",
      "{\"score\": 10, \"rationale\": \"Barely related.\"}\n",
      "{\"score\": 100, \"rationale\": \"Directly provides a recipe for baking a cake.\"}\n",
      "{\"score\": 100, \"rationale\": \"Directly provides a recipe for baking a cake.\"}\n",
      "{\"score\": 100, \"rationale\": \"Directly provides a recipe for baking a cake.\"}\n",
      "{\"score\": 0, \"rationale\": \"Unrelated to baking a cake.\"}\n",
      "{\"score\": 0, \"rationale\": \"Unrelated to baking a cake.\"}\n",
      "{\"score\": 0, \"rationale\": \"Unrelated to baking a cake.\"}\n",
      "How do I frost a layer cake so it’s perfectly smooth? => 98.33333333333333 [100, 100, 95] Directly addresses the topic of baking a cake.\n",
      "What’s the best way to sear a steak in cast iron? => 10.0 [10, 10, 10] Barely related.\n",
      "Step-by-step recipe for a moist chocolate sponge cake. => 100.0 [100, 100, 100] Directly provides a recipe for baking a cake.\n",
      "Explain the Roman Empire’s fall in one paragraph. => 0.0 [0, 0, 0] Unrelated to baking a cake.\n"
     ]
    }
   ],
   "source": [
    "import os, json, time, requests\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "OPENROUTER_API_KEY = \"{PUT_IN_KEY_HERE}\"\n",
    "#OPENROUTER_API_KEY = os.environ[\"OPENROUTER_API_KEY\"]\n",
    "ENDPOINT = \"https://openrouter.ai/api/v1/chat/completions\"\n",
    "\n",
    "JUDGE_SYSTEM = \"\"\"You are an impartial LLM judge.\n",
    "Score the USER prompt's **relevance to the topic**: \"{topic}\".\n",
    "Return a JSON object: {{\"score\": <integer 0-100>, \"rationale\": \"<one short sentence>\"}}.\n",
    "Scoring rubric:\n",
    "- 90–100: Directly about the topic.\n",
    "- 70–89: Mostly about the topic; minor drift.\n",
    "- 40–69: Tangential/related cooking/baking; limited link to the topic.\n",
    "- 10–39: Barely related.\n",
    "- 0–9: Unrelated or off-topic.\n",
    "Output ONLY valid JSON; no extra text.\"\"\"\n",
    "\n",
    "def _one_call(prompt: str,\n",
    "              topic: str = \"baking a cake\",\n",
    "              model: str = \"gpt-3.5-turbo\",\n",
    "              timeout_s: int = 60) -> Dict:\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        # Optional app headers shown in docs:\n",
    "        # \"HTTP-Referer\": \"https://your.app\", \"X-Title\": \"LLM Judge\"\n",
    "    }\n",
    "    body = {\n",
    "        \"model\": model,\n",
    "        \"response_format\": {\"type\": \"json_object\"},\n",
    "        \"temperature\": 0,            # deterministic judging\n",
    "        \"max_tokens\": 128,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": JUDGE_SYSTEM.format(topic=topic)},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "    }\n",
    "    r = requests.post(ENDPOINT, headers=headers, data=json.dumps(body), timeout=timeout_s)\n",
    "    r.raise_for_status()\n",
    "    msg = r.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "    print(msg) # Print the raw response\n",
    "    return json.loads(msg)\n",
    "\n",
    "def score_prompts(prompts: List[str],\n",
    "                  topic: str = \"baking a cake\",\n",
    "                  model: str = \"gpt-3.5-turbo\",\n",
    "                  per_prompt_votes: int = 1,\n",
    "                  sleep_between_s: float = 0.0) -> Dict[str, Dict[str, Optional[float]]]:\n",
    "    out = {}\n",
    "    for p in prompts:\n",
    "        votes = []\n",
    "        reasons = []\n",
    "        for _ in range(per_prompt_votes):\n",
    "            res = _one_call(p, topic=topic, model=model)\n",
    "            score = int(res.get(\"score\", 0))\n",
    "            votes.append(score)\n",
    "            reasons.append(res.get(\"rationale\", \"\"))\n",
    "            if sleep_between_s: time.sleep(sleep_between_s)\n",
    "        avg = sum(votes) / len(votes)\n",
    "        out[p] = {\n",
    "            \"scores\": votes,\n",
    "            \"avg\": avg,\n",
    "            \"min\": min(votes),\n",
    "            \"max\": max(votes),\n",
    "            \"rationales\": reasons,\n",
    "        }\n",
    "    return out\n",
    "\n",
    "# Example\n",
    "if __name__ == \"__main__\":\n",
    "    prompts = [\n",
    "        \"How do I frost a layer cake so it’s perfectly smooth?\",\n",
    "        \"What’s the best way to sear a steak in cast iron?\",\n",
    "        \"Step-by-step recipe for a moist chocolate sponge cake.\",\n",
    "        \"Explain the Roman Empire’s fall in one paragraph.\"\n",
    "    ]\n",
    "    results = score_prompts(prompts, topic=\"baking a cake\", model=\"gpt-3.5-turbo\", per_prompt_votes=3, sleep_between_s=0.2)\n",
    "    for k, v in results.items():\n",
    "        print(k, \"=>\", v[\"avg\"], v[\"scores\"], v[\"rationales\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1759567560391,
     "user": {
      "displayName": "Greg K.",
      "userId": "05140773900542914628"
     },
     "user_tz": -180
    },
    "id": "W15KX1uJAWn-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A9-nawtTZV_p"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMu5BQbyjekBOtRcur0sbaZ",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
